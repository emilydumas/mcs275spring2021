{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 9 Solutions\n",
    "\n",
    "## MCS 275 Spring 2021 - Emily Dumas\n",
    "### Solutions by Jennifer Vaccaro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "The main topics of this worksheet are:\n",
    "* Tree traversals\n",
    "* Set and defaultdict\n",
    "* CSV and JSON\n",
    "\n",
    "The main references for these topics are:\n",
    "* [Lecture 21 - Traversals](http://dumas.io/teaching/2021/spring/mcs275/slides/lecture21.html)\n",
    "* [Lecture 22 - set and defaultdict](http://dumas.io/teaching/2021/spring/mcs275/slides/lecture22.html)\n",
    "* [Lecture 23 - CSV and JSON](http://dumas.io/teaching/2021/spring/mcs275/slides/lecture23.html)\n",
    "\n",
    "The most useful files from the sample code repository are:\n",
    "* [binary.py](https://github.com/emilydumas/mcs275spring2021/blob/master/samplecode/trees/binary.py)\n",
    "* [bst.py](https://github.com/emilydumas/mcs275spring2021/blob/master/samplecode/trees/bst.py)\n",
    "* [treevis.py](https://github.com/emilydumas/mcs275spring2021/blob/master/samplecode/trees/treevis.py)\n",
    "* [treeutil.py](https://github.com/emilydumas/mcs275spring2021/blob/master/samplecode/trees/treeutil.py)\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Problem 1 is handled differently than the others:\n",
    "    * Tuesday discussion students: Problem 1 will be presented as an example at the start of discussion\n",
    "    * Thursday discussion students: Please complete Problem 1 before discussion and bring your solution\n",
    "* For the other problems:\n",
    "    * Work on these problems in discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  BST copier\n",
    "\n",
    "Write a function `bst_copy(T)` that takes a `BST` object `T` (with integer keys) and returns a copy of it, created from scratch by instantiating a new `BST` object and inserting the keys from `T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCS 275 Worksheet 9 Problem 1\n",
    "# Jennifer Vaccaro\n",
    "# I wrote this solution in collaboration with the Tuesday discussion section (thanks!)\n",
    "\n",
    "import bst\n",
    "import binary\n",
    "\n",
    "def copy_subtree(T, x):\n",
    "    \"\"\"If x != None, copies the key from x into a new node and adds it to a tree T. \n",
    "    Then recurse by copying the subtree descending from x's children.\"\"\"\n",
    "    if x == None:\n",
    "        return\n",
    "    # Create a new node with the same key as x. \n",
    "    new_x = binary.Node(x.key)\n",
    "    # Inserting the new node into T assigns the parent/children\n",
    "    T.insert(new_x)\n",
    "    # Then copy the subtree from x.left and x.right\n",
    "    copy_subtree(T,x.left)\n",
    "    copy_subtree(T,x.right)\n",
    "\n",
    "\n",
    "def bst_copy(T):\n",
    "    \"\"\"Returns a new tree with the same attributes/nodes as T\"\"\"\n",
    "    # Create a new empty tree\n",
    "    new_T = bst.BST()\n",
    "    # Call the recursive helper function to copy each of the nodes\n",
    "    copy_subtree(new_T, T.root)\n",
    "    return new_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original:\n",
      "                      <7>                      \n",
      "\n",
      "          <5>                     <12>         \n",
      "\n",
      "                            <10>        <16>   \n",
      "\n",
      "                                           <17>\n",
      "\n",
      "The copy:\n",
      "                      <7>                      \n",
      "\n",
      "          <5>                     <12>         \n",
      "\n",
      "                            <10>        <16>   \n",
      "\n",
      "                                           <17>\n",
      "\n",
      "Are they the same object? (Expect False.)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import treeutil\n",
    "import treevis\n",
    "\n",
    "T = treeutil.random_bst(6)\n",
    "T2 = bst_copy(T)\n",
    "print(\"The original:\")\n",
    "treevis.treeprint(T)\n",
    "print(\"The copy:\")\n",
    "treevis.treeprint(T2)\n",
    "print(\"Are they the same object? (Expect False.)\")\n",
    "print(T==T2)  # Since we didn't define an __eq__ method, this will return False\n",
    "              # unless T and T2 are different names for a single object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Accidentally quadratic\n",
    "\n",
    "Here is a function that takes two strings and returns the set of characters that appear in both strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_chars(s1,s2):\n",
    "    \"\"\"Return a set of all characters that are present in both\n",
    "    strings `s1` and `s2`.\"\"\"\n",
    "    common = set()\n",
    "    for c1 in s1:\n",
    "        if c1 in s2:\n",
    "            common.add(c1)\n",
    "    return common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this function is that it has an implicit nested for loop that performs `len(s1)*len(s2)` equality checks.  The expression\n",
    "```\n",
    "c1 in s2\n",
    "```\n",
    "is equivalent to the return value of this function:\n",
    "```\n",
    "def c1_in_s2():\n",
    "    for c2 in s2:\n",
    "        if c1==c2:\n",
    "            return True\n",
    "    return False\n",
    "```\n",
    "In the worst case, this function performs `len(s2)` checks, and it runs for each character of `s1`.  If `s1` and `s2` are each of length 50,000, then this becomes 2,500,000,000 equality checks.\n",
    "\n",
    "However, most strings don't have that many distinct characters, so it would be faster to:\n",
    "* Find all of the distinct characters in `s1` (and make a set out of them)\n",
    "* Find all of the distinct characters in `s2` (and make a set out of them)\n",
    "* Check which characters lie in both of these sets\n",
    "\n",
    "The time it takes to do this would be roughly proportional to `len(s1) + len(s2) + n1*n2` where `n1` is the number of distinct characters in `s1` and `n2` is the number of distinct characters in `s2`.  In most cases `n1` and `n2` will be bounded by a fixed constant (like `26`, if the strings only contain lower case letters of the alphabet), so the main contribution to the running time is proportional to the lengths of `s1` and `s2` individually, rather than their product.\n",
    "\n",
    "Here is an alternative `common_chars` function that uses this strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCS 275 Worksheet 9 Problem 3\n",
    "# Jennifer Vaccaro\n",
    "# I wrote this solution myself, in accordance with the syllabus\n",
    "\n",
    "def common_chars(s1,s2):\n",
    "    \"\"\"Return a set of all characters that are present in both\n",
    "    strings `s1` and `s2`.\"\"\"\n",
    "    # By first turning s1 and s2 into sets, we have fewer characters to compare.\n",
    "    # Then we can return the intersection\n",
    "    return set(s1) & set(s2)\n",
    "\n",
    "# Another syntax option would be 'return set(s1).intersection(s2)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works.  Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c', 'e', 'i', 'm', 's', 't'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_chars(\"mathematics\",\"computer science\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a timing study, showing it handles strings of length 50,000 with ease:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common characters:\n",
      "{'v', 'q', 'm', 'n', 'h', 'j', 'g', 'i', 't', 'l', 'k', 'p', 'd', 'r', 'e', 's', 'o', 'z', 'u', 'w', 'f'}\n",
      "\n",
      "Running time: 0.05 seconds\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "s1 = ''.join([ random.choice([\"edfghijklmnopqrstuvwxyzzzzzzzzzzzzzzzz\"]) for _ in range(50000) ])\n",
    "s2 = ''.join([ random.choice([\"abcedfghijklmnopqrstuvw\"]) for _ in range(50000) ]) + 'z'\n",
    "\n",
    "t_start = time.time()\n",
    "both = common_chars(s1,s2)\n",
    "t_end = time.time()\n",
    "\n",
    "print(\"Common characters:\")\n",
    "print(both)\n",
    "print(\"\\nRunning time: {:.2f} seconds\".format(t_end-t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Nobel prize winners data\n",
    "\n",
    "Download the JSON file of data about Nobel laureates from\n",
    "\n",
    "http://api.nobelprize.org/v1/laureate.json\n",
    "\n",
    "Write a Python program that reads this file and uses `defaultdict` to make and print a histogram of the institutional affiliations that appear in each prize category (e.g. in chemistry, how many nobel laureates have their institutional affiliation as \"University of California\"?  in peace, what institutional affiliation is listed for the largest number of laureates?)\n",
    "\n",
    "Poking around in the JSON data to figure out where the relevant information is stored will be the first step.  I suggest loading it into an object in the REPL and then exploring the list of keys, taking the value associated to one of those keys and listing its keys, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#PRIZES   INSTITUTION\n",
      "     14   University of California\n",
      "      8   MRC Laboratory of Molecular Biology\n",
      "      6   Harvard University\n",
      "      6   Howard Hughes Medical Institute\n",
      "      5   Berlin University\n",
      "      5   California Institute of Technology (Caltech)\n",
      "      5   Stanford University\n",
      "      4   University of Heidelberg\n",
      "      4   Eidgenössische Technische Hochschule (Swiss Federal Institute of Technology)\n",
      "      4   Rockefeller University\n",
      "      3   Stockholm University\n",
      "      3   Munich University\n",
      "      3   Goettingen University\n",
      "      3   University of Oxford\n",
      "      3   University of Cambridge\n",
      "      3   Columbia University\n",
      "      3   Cornell University\n",
      "      3   Yale University\n",
      "      3   Technion - Israel Institute of Technology\n",
      "      2   Sorbonne University\n",
      "      2   University of Zurich\n",
      "      2   Uppsala University\n",
      "      2   I.G. Farbenindustrie A.G.\n",
      "      2   Institut du Radium\n",
      "      2   Rockefeller Institute for Medical Research\n",
      "      2   Imperial College\n",
      "      2   University of Sussex\n",
      "      2   University of Texas\n",
      "      2   Purdue University\n",
      "      2   University of Southern California\n",
      "      2   Massachusetts Institute of Technology (MIT)\n",
      "      2   Rice University\n",
      "      2   Northwestern University\n",
      "      2   The Scripps Research Institute\n",
      "      2   Stanford University School of Medicine\n",
      "      1   University College\n",
      "      1   Landwirtschaftliche Hochschule (Agricultural College)\n",
      "      1   Victoria University\n",
      "      1   Leipzig University\n",
      "      1   Nancy University\n",
      "      1   Toulouse University\n",
      "      1   Kaiser-Wilhelm-Institut (now Fritz-Haber-Institut) für physikalische Chemie und Electrochemie\n",
      "      1   Graz University\n",
      "      1   London University\n",
      "      1   Technische Hochschule (Institute of Technology)\n",
      "      1   General Electric Company\n",
      "      1   Kaiser-Wilhelm-Institut (now Max-Planck-Institut) für Physik\n",
      "      1   Birmingham University\n",
      "      1   Kaiser-Wilhelm-Institut (now Max-Planck Institut) für Medizinische Forschung\n",
      "      1   Kaiser-Wilhelm-Institut (now Max-Planck-Institut) für Biochemie\n",
      "      1   Kaiser-Wilhelm-Institut (now Max-Planck Institut) für Chemie\n",
      "      1   University of Helsinki\n",
      "      1   Kiel University\n",
      "      1   Cologne University\n",
      "      1   National Institute for Medical Research\n",
      "      1   Rowett Research Institute\n",
      "      1   University of Freiburg\n",
      "      1   Staatliches Institut für makromolekulare Chemie (State Research Institute for Macromolecular Chemistry), Freiburg\n",
      "      1   Institute for Chemical Physics of the Academy of Sciences of the USSR\n",
      "      1   Lomonosov Moscow State University\n",
      "      1   Polarographic Institute of the Czechoslovak Academy of Science\n",
      "      1   Max-Planck-Institut für Kohlenforschung (Max-Planck-Institute for Carbon Research)\n",
      "      1   Institute of Technology\n",
      "      1   University of Oxford, Royal Society\n",
      "      1   University of Chicago\n",
      "      1   Max-Planck-Institut für Physikalische Chemie\n",
      "      1   Institute of Physical Chemistry\n",
      "      1   Royal Institution of Great Britain\n",
      "      1   University of Oslo\n",
      "      1   Institute for Biochemical Research\n",
      "      1   National Research Council of Canada\n",
      "      1   National Institutes of Health\n",
      "      1   Technical University\n",
      "      1   Université Libre de Bruxelles\n",
      "      1   Glynn Research Laboratories\n",
      "      1   Harvard University, Biological Laboratories\n",
      "      1   Kyoto University\n",
      "      1   The Medical Foundation of Buffalo\n",
      "      1   US Naval Research Laboratory\n",
      "      1   University of Toronto\n",
      "      1   Université Louis Pasteur\n",
      "      1   Collège de France\n",
      "      1   Du Pont\n",
      "      1   University of Texas Southwestern Medical Center at Dallas\n",
      "      1   Max-Planck-Institut für Biochemie\n",
      "      1   Max-Planck-Institut für Biophysik\n",
      "      1   University of Colorado\n",
      "      1   University of British Columbia\n",
      "      1   Max-Planck-Institut für Chemie\n",
      "      1   Aarhus University\n",
      "      1   University of Pennsylvania\n",
      "      1   University of Tsukuba\n",
      "      1   Nagoya University\n",
      "      1   Virginia Commonwealth University\n",
      "      1   Shimadzu Corp.\n",
      "      1   Johns Hopkins University School of Medicine\n",
      "      1   Institut Français du Pétrole\n",
      "      1   Fritz-Haber-Institut der Max-Planck-Gesellschaft\n",
      "      1   Marine Biological Laboratory (MBL)\n",
      "      1   Boston University Medical School\n",
      "      1   Weizmann Institute of Science\n",
      "      1   University of Delaware\n",
      "      1   Hokkaido University\n",
      "      1   Duke University Medical Center\n",
      "      1   Université de Strasbourg\n",
      "      1   Janelia Research Campus, Howard Hughes Medical Institute\n",
      "      1   Max Planck Institute for Biophysical Chemistry\n",
      "      1   German Cancer Research Center\n",
      "      1   Francis Crick Institute\n",
      "      1   Clare Hall Laboratory\n",
      "      1   Duke University School of Medicine\n",
      "      1   University of North Carolina\n",
      "      1   University of Strasbourg\n",
      "      1   University of Groningen\n",
      "      1   University of Lausanne\n",
      "      1   University of Missouri\n",
      "      1   Binghamton University, State University of New York\n",
      "      1   Asahi Kasei Corporation\n",
      "      1   Meijo University\n",
      "      1   Max Planck Unit for the Science of Pathogens\n"
     ]
    }
   ],
   "source": [
    "# MCS 275 Worksheet 9 Problem 3\n",
    "# Jennifer Vaccaro and Emily Dumas\n",
    "\"\"\"Creates a histogram of the schools affiliated with the Nobel Prize in Chemistry\"\"\"\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create an empty defaultdict with default value zero\n",
    "# (which is the value returned by function int)\n",
    "hist = defaultdict(int)\n",
    "\n",
    "# Open the file object. This syntax will automatically \n",
    "# close the file for you once the indentation block ends\n",
    "with open(\"laureate.json\",\"r\") as f:\n",
    "    data = json.load(f)\n",
    "    # Iterate through the laureates\n",
    "    for l in data[\"laureates\"]:\n",
    "        # Iterate through the prizes\n",
    "        for p in l[\"prizes\"]:\n",
    "            if p[\"category\"] != \"chemistry\":\n",
    "                # Skip if the prize category is not chemistry\n",
    "                continue \n",
    "            # Iterate through the affiliations\n",
    "            for a in p[\"affiliations\"]:\n",
    "                # Skip if the affiliation is not a dictionary\n",
    "                if not isinstance(a,dict):\n",
    "                    continue\n",
    "                # Add the school/institution name to the histogram.\n",
    "                # Because we are using defaultdict, if we attempt to access\n",
    "                # a key that doesn't exist, it will be created and given the\n",
    "                # value zero.\n",
    "                hist[a[\"name\"]] += 1\n",
    "\n",
    "\n",
    "# Now, hist maps institution names to prize counts in chemistry\n",
    "# We could just print the whole thing, but we can get a list in descending order\n",
    "# using dict.items() to get key-value pairs and then sorting by value.\n",
    "\n",
    "print(\"#PRIZES   INSTITUTION\")\n",
    "for institution, count in sorted(hist.items(),key=lambda x:-x[1]):\n",
    "    print(\"{:>7d}   {}\".format(count,institution))\n",
    "    \n",
    "# the argument key=lambda x:-x[1] in the call to `sorted` means sort by\n",
    "# second element of the tuple, in descending order.  The tuples are\n",
    "# (institution,prize_count) so this means the most prizes appear first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. CSV merge\n",
    "\n",
    "Write a program to merge any number of CSV files, so that each row in any of the input files becomes a row in the output file.  All of the input CSV files will have header rows.  If the CSV files have the same columns, this is of course easy.  But you should also handle the general case, where some columns may exist in multiple files, and others may be unique to a single file.  The output file should contain one column for each distinct column name that appears in any of the input files.\n",
    "\n",
    "Arrange it so your program `csvmerge.py` accepts all the input filenames as command line arguments.  The last command line argument is the name of the output file that should be created.\n",
    "\n",
    "For example, you might use a command like\n",
    "```\n",
    "python3 csvmerge.py a.csv b.csv c.csv out.csv\n",
    "```\n",
    "with `a.csv` containing:\n",
    "```\n",
    "name,age,favorite\n",
    "Melissa,52,vanilla\n",
    "Jonah,24,strawberry\n",
    "```\n",
    "and `b.csv` containing:\n",
    "```\n",
    "name,major\n",
    "Josefina,falconry\n",
    "David,phrenology\n",
    "```\n",
    "and `c.csv` containing:\n",
    "```\n",
    "age,major\n",
    "5,bubbles\n",
    "11,chess\n",
    "```\n",
    "In which case the program should create `out.csv` containing:\n",
    "```\n",
    "name,age,favorite,major\n",
    "Melissa,52,vanilla,\n",
    "Jonah,24,strawberry,\n",
    "Josefina,,,falconry\n",
    "David,,,phrenology\n",
    ",5,,bubbles\n",
    ",11,,chess\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCS 275 Worksheet 9 Problem 4\n",
    "# Jennifer Vaccaro and Emily Dumas\n",
    "\"\"\"csvmerge reads a set of input csv and writes their combined \n",
    "data into a single output csv\"\"\"\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# This code keeps me from accidentally emptying out this text file, \n",
    "# needs at least 2 command line args\n",
    "if len(sys.argv) < 3:\n",
    "    raise Exception(\"Usage: {} INPUTFILES OUTPUTFILE\".format(sys.argv[0]))\n",
    "\n",
    "# Iterate through the input files, adding the column names to a set\n",
    "# At this point we don't look at any of the input data---only the\n",
    "# header row.\n",
    "columns = set()\n",
    "for csv_fn in sys.argv[1:-1]:\n",
    "    with open(csv_fn,newline=\"\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for col in reader.fieldnames:\n",
    "            columns.add(col)\n",
    "            \n",
    "# Note: because we use a set to store the column names, the columns\n",
    "# may appear in a different order in the output file than they do in\n",
    "# any input file.  If order were important, we'd need to use a data\n",
    "# structure other than a set to store the column names.\n",
    "\n",
    "# Now, open the output file, and write the data from each input file into the output file.\n",
    "# Use the combined input fieldnames.  This is the second time each input file is opened.\n",
    "out_fn = sys.argv[-1]\n",
    "with open(out_fn,\"w\") as out_file:\n",
    "    writer = csv.DictWriter(out_file, fieldnames=list(columns))  # ieldnames must be a list\n",
    "    writer.writeheader()\n",
    "    # Iterate through our input files again, and write each row of them to the output file.\n",
    "    for csv_fn in sys.argv[1:-1]:\n",
    "        with open(csv_fn,newline=\"\") as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "            for row in reader:\n",
    "                writer.writerow(row)  # row may not have all the keys that `writer` expects\n",
    "                                      # but that's ok; DictWriter allows missing keys and\n",
    "                                      # fills in an empty output field for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. USPS facilities data conversion\n",
    "\n",
    "Download and unzip this CSV file listing facilities (mostly post offices) owned by the USPS in the 50 states:\n",
    "* [usps_facilities.zip](https://dumas.io/teaching/2021/spring/mcs275/data/usps_facilities.zip)\n",
    "\n",
    "Take a look at the file to get a sense of how it is structured.  (You may need to look back at it later to determine which column headings are relevant to the tasks below.)\n",
    "\n",
    "Now, write a program that uses the `csv` module to read this file and process it into a hierarchy of summary data that is written to a JSON file (using the `json` module).  The output should have the following hierarchy:\n",
    "\n",
    "* At the top level, there is one key for each 2-letter state abbreviation (e.g. \"IL\")\n",
    "    * The value associated to a state abbreviation is an object whose keys are the names of postal districts that have facilities in that state (e.g. \"Central Illinois\")\n",
    "        * The value associated to a postal district is an object whose keys and values are as follows:\n",
    "            * key \"total facilities\" with value the number of facilities in that state and postal district\n",
    "            * key \"post offices\" with value the number of facilities in that state and postal district that have facility type (column `FDB Facility Type (All)`) equal to `Post Office`.\n",
    "\n",
    "For example, the output file might begin\n",
    "```\n",
    "{ \"IL\": { \"Central Illinois\": { \"total facilities\": 165, \"post offices\": 144 }, ... \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCS 275 Worksheet 9 Problem 5\n",
    "# Emily Dumas\n",
    "\"\"\"Read CSV of USPS facilities and compile summary statistics\n",
    "by state and district in JSON output file.\"\"\"\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import collections\n",
    "\n",
    "infn = \"usps_facilities.csv\"\n",
    "outfn = \"usps_facility_summary.json\"\n",
    "\n",
    "def district_summary_factory():\n",
    "    \"\"\"Default value for a district's summary data\"\"\"\n",
    "    return { \"total facilities\":0, \"post offices\": 0 }\n",
    "\n",
    "def state_summary_factory():\n",
    "    \"\"\"Default value for a state's summary data\"\"\"\n",
    "    return collections.defaultdict(district_summary_factory)\n",
    "\n",
    "summary = collections.defaultdict(state_summary_factory)\n",
    "\n",
    "# After all of this setup, we now have an object for storing\n",
    "# the summary data where we never need to create keys. A line\n",
    "# like\n",
    "#   summary[\"IL\"][\"Central Illinois\"][\"post offices\"] += 1\n",
    "# will create everything needed and increment the count.\n",
    "\n",
    "with open(infn,newline=\"\") as infile:\n",
    "    rdr=csv.DictReader(infile)\n",
    "    for facility in rdr:\n",
    "        # Put key elements of the facility data\n",
    "        # into conveniently named variables\n",
    "        state = facility[\"ST\"]\n",
    "        district = facility[\"District\"]\n",
    "        is_po = facility[\"FDB Facility Type (All)\"]==\"Post Office\"\n",
    "\n",
    "        # Record what we want to know about this facility\n",
    "        summary[state][district][\"total facilities\"] += 1\n",
    "        if is_po:\n",
    "            summary[state][district][\"post offices\"] += 1\n",
    "\n",
    "with open(outfn,\"w\") as outfile:\n",
    "    json.dump(summary,outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting output JSON file is very long; here's the part of it for Florida and Georgia (with extra whitespace and indenting added for readability):\n",
    "```\n",
    "...\n",
    "  \"FL\": {\n",
    "    \"Suncoast\": {\n",
    "      \"total facilities\": 187,\n",
    "      \"post offices\": 162\n",
    "    },\n",
    "    \"Gulf Atlantic\": {\n",
    "      \"total facilities\": 104,\n",
    "      \"post offices\": 80\n",
    "    },\n",
    "    \"South Florida\": {\n",
    "      \"total facilities\": 102,\n",
    "      \"post offices\": 90\n",
    "    }\n",
    "  },\n",
    "  \"GA\": {\n",
    "    \"Atlanta\": {\n",
    "      \"total facilities\": 143,\n",
    "      \"post offices\": 114\n",
    "    },\n",
    "    \"Gulf Atlantic\": {\n",
    "      \"total facilities\": 104,\n",
    "      \"post offices\": 86\n",
    "    },\n",
    "    \"Tennessee\": {\n",
    "      \"total facilities\": 9,\n",
    "      \"post offices\": 9\n",
    "    }\n",
    "  },\n",
    "...\n",
    "```\n",
    "(There's no typo there; some of the post offices in Georgia are in a district that USPS calls \"Tennessee\".  That district also contains all of the post offices in the state of Tennessee.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Kafkaesque *and* Tolstoyesque\n",
    "\n",
    "What words appear in both *War and Peace* by Leo Tolstoy and *The Metamorphosis* by Franz Kafka?\n",
    "\n",
    "Answer this for the english translations of these novels available as UTF-8 plain text from Project Gutenberg:\n",
    "* [War and Peace](http://www.gutenberg.org/files/2600/2600-0.txt)\n",
    "* [Metamorphosis](http://www.gutenberg.org/cache/epub/5200/pg5200.txt)\n",
    "\n",
    "(You should look at the files in a browser to see the format; there is a header and a footer, with the actual text in between.)\n",
    "\n",
    "Counting words is tricky, and there are a lot of edge cases, so let's impose these rules:\n",
    "* A word means a maximal block of consecutive characters that are alphabetic\n",
    "    * A character `c` is considered alphabetic if `c.isalpha()` returns `True`.  The method `isalpha` of class `str` is documented [here](https://docs.python.org/3/library/stdtypes.html#str.isalpha)\n",
    "* Words are considered the same if they differ only in capitalization\n",
    "* Chapter headings and roman numerals count as words (to save you the trouble of devising code to recognize and exclude these)\n",
    "\n",
    "These files contain accented characters and various exotic punctuation (e.g. proper start and end quotation marks, not just the symmetric ones on your keyboard).  As a result, to find words and ignore punctuation it will be easiest to use a positive test (`.isalpha` decides what is a letter) instead of a negative one (e.g. a hard-coded list of punctuation characters to convert to spaces before splitting).\n",
    "\n",
    "*Having finished the last exercise of the worksheet, you can now wake from these troubled dreams to find yourself transformed into a more skilled programmer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in common: 2300\n",
      "\n",
      "Longest shared word(s):\n",
      "incomprehensible\n",
      "\n",
      "All shared words in alphabetical order:\n",
      "a,abandoned,abandoning,ability,able,about,above,abruptly,absolutely,accept,acceptance,accumulated,accurately,accusations,accuse,accused,achieved,achievements,aching,acquired,across,act,action,actually,add,added,address,adjoining,admit,advantage,advice,advise,affairs,affection,afraid,after,afterwards,again,against,age,aggrieved,agitated,ago,agree,agreed,ah,ahead,aim,air,alarm,alarmed,alert,all,allow,allowed,almonds,almost,alone,along,already,also,although,altogether,always,am,amazed,amazement,among,amount,an,ancient,and,anew,anger,angle,angry,animal,anna,announcement,annoyed,another,answer,answered,answering,anticipation,anxiety,anxious,anxiously,any,anyone,anything,anyway,anywhere,apart,appeals,appear,appearance,appeared,apple,apples,appreciated,appreciation,approached,appropriate,approved,arc,arduous,are,area,aren,arm,armchair,arms,army,around,arrange,arranged,arrangement,arrive,arrived,as,aside,ask,asked,asleep,assailed,assistant,assuming,assure,astonished,at,ate,attack,attempted,attention,attentive,attract,attracted,attributed,austere,authority,available,avoid,awake,aware,away,awful,awkwardly,awoke,back,backs,backwards,bad,badly,balance,balls,banging,banister,bank,bare,barren,basis,be,bear,beards,bearing,beautiful,became,because,become,becoming,bed,bedding,bedroom,beds,been,beer,before,beg,began,begged,begging,begin,beginning,begun,behalf,behaved,behind,being,beings,believe,believed,believing,belly,belonging,below,bend,benefit,bent,beside,besides,best,better,between,beyond,big,bit,bite,bitter,black,blanket,bleeding,blew,block,blossoming,blow,blown,blue,body,bolted,bombard,bone,boned,bones,boots,boss,both,bother,bottle,bottles,bottom,bound,bow,bowed,bowl,box,boy,bravely,bread,breadth,break,breakfast,breakfasts,breast,breath,breathe,breathing,brief,briefly,bring,bringing,broad,broke,broom,brother,brought,brown,budget,building,built,bulging,bulk,burdensome,burning,bushy,business,but,buttons,by,c,call,called,calling,calls,calm,calmed,calmer,calmly,calmness,came,can,cannot,cap,care,career,carefree,careful,carefully,carelessly,carpet,carried,carry,carrying,carved,case,cash,catch,catching,caught,cause,caused,causing,caustic,caution,cautious,ceiling,centre,certainly,certainty,chair,chairs,chance,change,changed,changes,changing,chaos,chase,cheaper,checked,checking,cheek,cheeks,cheer,cheese,chest,chewing,chief,child,childish,chilly,chin,choice,choose,chose,chosen,christmas,chuck,circumstances,city,claim,clapping,clean,cleaning,clear,cleared,clearer,clearly,clenched,clerk,clever,climb,climbing,clock,close,closed,closely,closer,closing,clothes,clumsily,clumsy,coat,coats,coffee,cold,collapsed,collar,collection,combed,come,comes,comfort,comfortable,comfortably,coming,comment,commerce,commercial,communicated,communication,companions,company,compared,complaint,complaints,complete,completely,complicated,comprehension,concealed,concentrated,concentration,concern,concerned,conclusion,conclusions,condemned,condition,conditions,confidence,confident,confirm,confirmation,confused,confusion,congratulated,connection,connections,conscience,consciousness,conservatory,consider,considerate,consideration,considerations,considered,considering,consisting,contact,containing,content,contented,contents,continually,continue,continued,continuing,continuously,contrary,contrast,control,controlled,conversation,conversations,converted,convinced,convulsive,cook,cooked,cooking,cool,copy,corner,corpse,couch,cough,coughed,coughing,could,couldn,count,countless,country,couple,courage,course,courtesy,cover,covered,covering,covers,crack,crash,crawl,crawled,crawling,cried,cross,crossed,crouched,cry,crying,curiosity,curious,current,curse,curtains,customers,cut,d,damage,damp,dampness,danced,danger,dare,dared,daring,dark,darkness,daughter,david,dawning,day,days,dead,deal,dear,death,debt,decide,decided,decision,decisions,decisive,decisively,declare,declared,deep,deeper,deeply,defend,definite,definitely,delay,delaying,delighted,demands,denuded,depended,deranged,describing,desk,despair,desperate,despite,destination,destroyed,detail,determination,determined,devoted,did,didn,different,differently,difficult,difficulty,diligently,dinner,direct,direction,directions,directly,dirt,dirty,disappear,disappeared,disappointed,disappointment,discard,disconcerted,discovery,discussed,disgust,dish,dishes,dismissal,dismissed,dissuade,distance,distinct,distinctly,distinguish,distract,distress,disturb,disturbed,disturbing,divided,do,doctor,document,doing,dominated,don,done,door,doors,doorway,double,doubt,down,dozing,drag,dragged,drank,draught,draw,drawers,drawn,dream,dreams,dressed,drew,dried,drink,drive,driven,driving,drop,dropped,drops,drove,dry,dull,dung,during,dust,duties,duty,dying,each,ear,earlier,early,earn,earned,earnest,ears,earth,ease,easier,easily,easy,eat,eaten,eating,echo,edge,edges,effect,effects,effort,efforts,eight,either,elastic,elderly,electric,else,emerged,emotion,emphatically,employer,emptied,empty,emptying,enable,encouraged,end,endearments,ended,endless,endure,enemy,engrossed,enjoy,enjoyed,enormous,enough,enter,entered,entering,entertain,entertaining,enthusiasm,enthusiastic,entire,entirely,entitled,entrance,entrusted,enveloped,equipment,errand,especially,establish,establishment,even,evening,evenings,events,eventually,ever,every,everyone,everything,everywhere,exact,exactly,exaggerated,examination,example,exceptionally,excessive,exchange,excited,excitedly,excitement,excuse,exhausted,exhaustion,expectations,expected,expecting,expects,expense,experience,experienced,explain,explained,explaining,explanation,explanations,express,expression,expressive,extreme,extremely,eyebrows,eyed,eyes,face,fact,fail,failed,failure,faint,faithful,fall,fallen,falling,familiar,family,fancy,far,fashion,fast,father,fear,feared,feather,feeble,feed,feeding,feel,feeling,feelings,feet,fell,felt,fetch,few,fiery,fifteen,filled,final,finally,finances,find,finger,fingers,finished,firm,firmly,first,fist,fists,fitted,five,fixed,flank,flapped,flat,fled,flee,flesh,flew,flight,floating,floor,floundering,flow,flowed,flowers,flown,fluid,fluttered,flying,fog,folds,followed,following,fond,food,fools,foot,footsteps,for,force,forced,forefinger,forehead,forget,forgetting,forgive,forgot,forgotten,forks,form,former,formerly,forth,fortunately,forward,forwards,found,four,fourteen,frame,franz,free,freed,freedom,freely,french,frenzy,frequent,frequently,fresh,friend,friendly,friends,frightening,fro,from,front,froze,fruit,full,fully,functions,funny,fur,furniture,further,future,gain,gained,game,gas,gather,gave,gazing,gentle,gentleman,gentlemen,gently,get,gets,getting,gifted,girl,girls,give,given,giving,glad,glance,glanced,glances,glancing,glass,gleeful,go,god,goes,going,gold,gone,good,goodness,gossip,got,gradually,grand,grant,grasp,gratitude,great,greatest,greatly,greedily,greeting,grin,groaning,ground,groundless,grounds,gruff,guessed,guest,gushed,habit,had,hadn,hair,hairs,half,hall,halt,hand,handle,hands,hang,hanging,happen,happened,happening,happy,hard,harder,hardest,hardly,harm,harmed,harmful,harshly,has,hat,hats,have,haven,having,hazard,he,head,heads,healed,healthy,hear,heard,hearer,hearing,heart,heaven,heavily,heaving,heavy,held,hell,help,helped,helping,helpless,helplessly,helplessness,her,here,herself,hidden,hide,high,highly,him,himself,hinder,his,hiss,hissed,hissing,hit,hitting,hold,holding,holes,holiday,home,homes,honour,hope,hoped,hopes,hoping,horrible,hospital,hostile,hot,hotel,hour,hours,house,household,housekeeper,how,however,hugged,human,humbly,humility,hundred,hung,hunger,hungry,hurried,hurriedly,hurry,hurrying,hurt,i,idea,ideas,if,ignoring,ii,iii,ill,illness,imagination,imagine,imagined,imagining,imaginings,immaculate,immediate,immediately,immobile,impatient,impatiently,impeding,imploring,important,impose,impossible,impression,improvement,in,inaccessible,incapable,inch,incomprehensible,incredible,indeed,indicating,indifferent,individual,infant,inflamed,influence,inherited,injured,injuries,injuring,injury,inner,innocent,inseparably,inside,insist,insistent,instance,instead,instrument,intended,intending,intent,intention,intentions,intercede,interest,interrupted,into,introduction,invalid,invariably,investigate,invisible,involved,irregular,irritably,irritation,is,isn,it,its,itself,jaw,jaws,job,join,joined,joining,joy,judge,jump,jumped,just,keep,keeping,kept,key,keys,kicked,kind,kiss,kissed,kitchen,knees,knew,knife,knives,knock,knocked,knocking,know,knowing,known,knows,laboriously,lack,lacking,lad,lady,lain,lamp,lamps,landed,landing,lap,large,last,lasted,late,later,latest,laugh,laughed,lay,laying,lazy,lead,leader,leading,leaned,leaning,leapt,learn,learned,least,leather,leave,leaves,leaving,led,left,leg,legs,length,less,let,letters,letting,lid,lie,lieutenant,life,lifelessly,lift,lifted,light,lighter,lightly,like,liked,likely,likes,limped,limply,line,lines,lips,listen,listened,listening,lit,little,live,lived,livelier,lively,lives,living,ll,load,lock,locked,locking,lodged,long,longer,longing,look,looked,looking,lose,losing,loss,lost,lot,lots,loud,loudest,loudly,love,lovely,lover,lower,lungs,lurched,luxury,lying,m,mad,made,maid,main,maintain,make,makes,making,man,managed,many,march,marriage,martyr,matter,matters,maybe,me,meal,meals,mean,meant,meantime,meanwhile,meat,mechanical,medical,medicine,meet,melancholy,member,members,memory,men,mention,mentioned,merely,met,method,midday,middle,might,mild,milk,mind,mingle,mingled,minutes,miracle,misfortune,misfortunes,miss,missed,mistake,mistakes,mixed,mixing,modest,moment,moments,money,monogram,monotonous,monster,month,months,mood,more,moreover,morning,most,mother,mothers,motionless,mouth,move,moved,movement,movements,moving,mr,much,muffled,music,must,muster,my,nap,narrow,natural,naturally,near,nearer,nearest,nearly,necessary,neck,necklace,need,needed,needle,needn,neglected,never,new,news,newspapers,next,nice,night,no,nobody,nodded,nodding,noise,nonsense,nor,noses,nostrils,not,notes,nothing,notice,noticeable,noticeably,noticed,noticing,nourishment,now,nowadays,nuisance,numb,number,numerous,o,object,observant,observed,obsessed,obstinate,obvious,obviously,occasionally,occupied,occur,occurred,of,off,offer,offered,office,often,oh,old,on,once,one,ones,only,onto,open,opened,opening,openly,opens,opinion,opportunity,opposite,oppressed,or,ordeal,order,ostrich,other,others,ought,our,ourselves,out,outside,outstretched,over,overcoat,overcome,overnight,owed,own,pack,packed,page,paid,pain,pained,painful,painfully,pair,pale,pane,pangs,panic,panting,pantry,paper,parents,part,particular,particularly,partly,passed,past,patch,patience,patient,pauses,pay,paying,peace,peaceful,peacefully,peculiar,peered,peering,peg,pen,people,perfect,perform,performance,perhaps,periods,permission,person,personally,persuade,persuaded,picked,picture,piece,pieces,piercing,piled,pillows,pity,place,placed,places,plaintively,plan,planned,plans,plates,play,played,player,playing,pleasant,please,pleased,pleasure,pleasures,plenty,pockets,point,pointing,polished,politeness,poor,position,positions,possibilities,possible,pot,potatoes,poured,pouring,powerful,practical,practice,prejudice,premises,preoccupied,preparation,prepared,present,press,pressed,pressure,pretend,prevail,prevent,prevented,previous,price,pride,principal,private,probably,proceed,produced,progress,promise,proper,properly,propped,protect,protruding,proud,prove,provide,provincial,public,pull,pulled,pulling,punished,pure,purpose,pursue,push,pushed,pushing,put,putting,quarrel,quarter,question,quick,quicker,quickly,quiet,quieter,quietly,quite,quivering,rag,rage,rain,raise,raised,raising,raisins,ran,rather,rattling,re,reach,reached,reaching,read,reading,reads,ready,real,really,rearranging,reason,reasons,receipt,receive,received,recent,recently,recommend,recommendation,recover,red,reduce,reduced,refrained,refuse,refused,refusing,regained,regard,regret,regular,regularly,related,relaxed,released,reliable,relieved,remain,remained,remains,remarkable,remember,remembered,remind,reminder,remove,removed,removing,rented,repeated,repeatedly,repelled,replied,reply,report,reported,representative,reproach,reproaches,repugnant,request,required,resistance,resolve,resolved,resounding,respect,responded,response,responsibility,responsible,rest,rested,result,retreating,return,revolting,revulsion,rid,right,ring,risked,robust,rock,rocked,roll,rolled,room,rooms,rotten,round,rounded,rows,rubbed,rubbing,run,running,rush,rushed,rushing,s,sacked,sacrifice,sacrificed,sad,saddened,sadly,safe,said,saints,sake,sales,salesman,salt,same,samples,sank,sat,satisfaction,satisfied,sauce,save,saved,saving,saw,say,saying,scar,school,scraped,scraping,scratching,scream,screamed,screaming,screams,scurry,seat,seats,second,secret,sections,secure,see,seem,seemed,seeming,seen,seized,seldom,selection,self,selfish,send,sending,sense,senses,sensible,sensitive,sent,separated,serious,seriously,serve,service,set,seven,seventeen,several,sew,sewing,shaken,shaking,shall,shame,sharply,she,sheet,shiny,shivering,shock,shocked,shone,shook,shop,short,should,shoulder,shoulders,shout,shouted,shouting,shouts,shove,shoved,show,showed,showing,shown,shrugged,shudder,shut,shy,sick,side,sides,sideways,sighing,sighs,sight,sign,silence,silent,sill,similar,simple,simply,simultaneously,since,sink,sir,sister,sit,sits,sitting,situation,six,sixteen,size,skirt,skirts,sky,slammed,slamming,sleep,sleepiness,sleeping,sleeve,slept,sliding,slight,slightest,slightly,slipped,slow,slowly,slumber,small,smaller,smart,smell,smelling,smile,smiled,smoke,smoking,smooth,snapped,snapping,snuffling,so,sofa,soft,softened,sold,soles,solid,some,somebody,someone,something,sometimes,somewhat,somewhere,son,soon,sooner,sorrows,sort,sorted,sought,sound,sounded,sounding,source,space,spare,speak,speaking,special,speech,spend,spent,spin,splashed,splinter,spoke,spoken,spot,spots,spread,spring,sprung,spying,square,squarely,squeaking,staff,staggered,staircase,stairs,stamped,stamping,stand,standing,stared,staring,start,started,starting,startled,startling,starve,state,station,stay,stayed,staying,steadily,steady,steaming,step,stepped,steps,stick,sticking,sticks,stiff,still,stillness,stone,stood,stop,stopped,straight,strain,strained,strangeness,stranger,strangers,street,streets,strength,strenuous,stretch,stretched,stretching,strict,strides,strike,strikes,striking,strong,strongly,struck,structure,struggled,struggling,stubborn,stuck,studies,stuff,stumbling,stupid,subdued,subject,subordinates,subtleties,succeeded,success,such,sucking,sudden,suddenly,suffer,suffered,suffering,sufficient,sufficiently,suffocating,suggest,suggested,suitable,sunday,sunk,sunshine,superfluous,superior,supernatural,supported,suppose,suppress,sure,surely,surplus,surprise,surprising,suspicious,swallow,swayed,sweep,sweetly,swept,swing,swinging,swishing,sword,swore,sympathy,t,table,take,taken,takes,taking,talk,talking,tall,task,taste,tearfully,tears,teeth,tell,telling,temporarily,tempted,ten,tender,terrible,terribly,test,than,thank,thanking,thanks,that,the,their,them,themselves,then,there,thereby,therefore,these,they,thick,thin,thing,things,think,thinking,thinks,this,thoroughly,those,though,thought,thoughtfully,thoughts,threads,threat,threatened,three,threw,through,throughout,throw,throwing,thrown,thunder,thus,tickle,tidied,tidy,tightly,till,time,times,tip,tips,tired,to,today,toe,together,told,tolerate,tone,tonight,too,took,tools,toothless,top,tops,tortured,tossed,total,totally,touched,toward,tower,town,traces,train,transferring,transformed,translated,transport,tray,treated,trembling,tremendous,tremor,tried,trip,troubled,trouser,true,trusted,try,trying,tug,tugged,turn,turned,turning,twice,twisted,two,unable,unbearable,unchanged,uncomfortable,uncontrolled,under,underneath,understand,understanding,understood,undisturbed,uneasy,unexpected,unfastened,unfolded,unfortunate,unfortunately,unhappy,unheard,uniform,uniting,unknown,unless,unlike,unlocked,unnatural,unnecessary,unpleasant,unpleasantness,unsatisfactory,untidiness,until,untouched,unused,unusual,unusually,unwell,up,upright,upset,upwards,urge,urged,us,use,used,useless,using,usual,usually,vague,vain,value,various,ve,very,victim,view,vigorously,vile,violence,violin,visible,visit,visitor,voice,volume,wait,waited,waiting,wake,walk,walked,walking,wall,walls,want,wanted,wanting,wants,warm,warmth,warning,was,washing,wasn,waste,wasted,wasting,watch,watched,watching,water,watering,waved,way,ways,we,weak,weaker,wealth,wearily,wearing,weather,week,weeks,weight,well,went,wept,were,what,whatever,when,whenever,where,whereabouts,whereas,whether,which,while,whims,whisper,whispered,whispering,whistled,white,who,whole,whom,whose,why,wide,widow,wife,wild,will,willing,wind,window,windowpanes,windows,wink,wipe,wiped,wisdom,wise,wish,wishing,with,withdraw,withdrew,within,without,withstand,woke,woman,women,won,wondered,wooden,word,words,work,worked,working,world,worn,worried,worries,worry,worst,would,wouldn,wrapped,wrenching,write,writing,wrong,wrote,year,years,yes,yesterday,yet,you,young,your,yourself\n"
     ]
    }
   ],
   "source": [
    "# MCS 275 Worksheet 9 Problem 6\n",
    "# Jennifer Vaccaro and Emily Dumas\n",
    "\"\"\"Finds the shared words between 'War and Peace' by Leo Tolstoy \n",
    "and 'Metamorphosis' by Franz Kafka\"\"\"\n",
    "\n",
    "# Set the names of the text files\n",
    "tolstoy_fn = \"tolstoy.txt\"\n",
    "kafka_fn = \"kafka.txt\"\n",
    "\n",
    "def lower_words(s):\n",
    "    \"\"\"Splits out a string s into lower case alphabet words by \n",
    "    converting all other characters to spaces then splitting\"\"\"\n",
    "    new_s = \"\"\n",
    "    for c in s:\n",
    "        if c.isalpha():\n",
    "            new_s += c.lower()\n",
    "        else:\n",
    "            new_s += \" \"\n",
    "    return new_s.split()\n",
    "\n",
    "def ebook_word_set(fobj):\n",
    "    \"\"\"Return a set containing the distinct words in a project\n",
    "    Gutenberg ebook text file.  Only counts words between the\n",
    "    start and end markers that these files include.\"\"\"\n",
    "    words = set()\n",
    "    watching = False # Indicator whether we're in the main text yet.\n",
    "    for line in f:\n",
    "        if line.startswith(\"*** START OF THIS PROJECT GUTENBERG EBOOK\"):\n",
    "            watching = True # We hit the start marker, so start counting words\n",
    "            continue        # on the *next* line.\n",
    "        if not watching:\n",
    "            continue\n",
    "        if line.startswith(\"End of the Project Gutenberg EBook\"):\n",
    "            break # we hit the end marker.  stop.\n",
    "        \n",
    "        # We are between the start and end marker.\n",
    "        # Add all words on this line to the set of words seen so far\n",
    "        words.update(lower_words(line))\n",
    "    return words\n",
    "    \n",
    "\n",
    "with open(tolstoy_fn,\"r\",encoding=\"utf8\") as f:\n",
    "    tolstoy_words = ebook_word_set(f)\n",
    "\n",
    "with open(kafka_fn,\"r\",encoding=\"utf8\") as f:\n",
    "    kafka_words = ebook_word_set(f)\n",
    "\n",
    "# The intersection of these sets is the set of shared words\n",
    "shared_words = tolstoy_words & kafka_words\n",
    "print(\"Total words in common:\",len(shared_words))\n",
    "lmax = max( [len(w) for w in shared_words] )\n",
    "print(\"\\nLongest shared word(s):\")\n",
    "print(\",\".join(sorted([w for w in shared_words if len(w)==lmax])))\n",
    "print(\"\\nAll shared words in alphabetical order:\")\n",
    "print(\",\".join(sorted(shared_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revision history\n",
    "\n",
    "* 2021-03-12 - Initial publication"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}